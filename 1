json build array fix for SQLite
# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
#
# On branch feature/sqlite
# Changes to be committed:
#	modified:   adbc/backends/sqlite.py
#	modified:   adbc/store/table.py
#	modified:   adbc/zql/builders/postgres.py
#	modified:   adbc/zql/parsers/sql.py
#	modified:   tests/integration/test_info.py
#
# ------------------------ >8 ------------------------
# Do not modify or remove the line above.
# Everything below it will be ignored.
diff --git a/adbc/backends/sqlite.py b/adbc/backends/sqlite.py
index f9361ab..0d71f64 100644
--- a/adbc/backends/sqlite.py
+++ b/adbc/backends/sqlite.py
@@ -32,6 +32,10 @@ def md5sum(t):
     return hashlib.md5(t).hexdigest()
 
 
+def json_build_array(*args):
+    return json.dumps(args)
+
+
 class SqlitePoolContext(object):
     def __init__(self, url):
         self.url = url
@@ -106,6 +110,7 @@ class SqliteBackend(DatabaseBackend):
             kwargs['isolation_level'] = None
         db = await connect(*args, **kwargs)
         await db.create_function('md5', 1, md5sum)
+        await db.create_function('json_build_array', -1, json_build_array)
         db.row_factory = Row
         return db
 
diff --git a/adbc/store/table.py b/adbc/store/table.py
index 5fd2f75..a8a263f 100644
--- a/adbc/store/table.py
+++ b/adbc/store/table.py
@@ -82,7 +82,6 @@ class Table(WithScope, Loggable):
         self.name = name
         self.verbose = verbose
         self.parent = self.namespace = namespace
-        self.database = namespace.database
         self.alias = alias or name
         self.tag = tag
 
@@ -107,6 +106,11 @@ class Table(WithScope, Loggable):
     def init_sequence(self):
         pass
 
+
+    @property
+    def database(self):
+        return self.namespace.database
+
     def init_table(self):
         """Normal table initializer"""
         # in one loop, create a two-way values binding for these properties:
@@ -115,7 +119,7 @@ class Table(WithScope, Loggable):
         # - unique: based on unique constraints
         # - related: based on foreign key constraints
         constraints = self.constraints
-        pks = self.pks = get_pks(constraints) or {}
+        self.pks = get_pks(constraints) or {}
         uniques = self.uniques = get_uniques(constraints)
         fks = self.fks = get_fks(constraints)
         for name, column in self.columns.items():
@@ -133,20 +137,20 @@ class Table(WithScope, Loggable):
                     column["sequence"] = column.get("sequence", False)
 
             if column.get("primary"):
-                if name not in pks:
+                if name not in self.pks:
                     primary = column.get("primary")
                     constraint_name = (
                         primary
                         if isinstance(primary, str)
                         else f"{self.name}__{name}__pk"
                     )
-                    pks[name] = constraint_name
+                    self.pks[name] = constraint_name
                     constraints[constraint_name] = G('constraint',
                         type=PRIMARY,
                         columns=[name]
                     )
             else:
-                column["primary"] = pks.get(name, False)
+                column["primary"] = self.pks.get(name, False)
             if column.get("unique"):
                 if name not in uniques:
                     unique = column.get("unique")
@@ -185,14 +189,14 @@ class Table(WithScope, Loggable):
                     }
             else:
                 column['related'] = fks.get(name, None)
-            if not pks:
-                self.pks = {
-                    name: True for name in self.column_names
-                }
-            if len(self.pks) == 1:
-                self.pk = next(iter(self.pks))
-            else:
-                self.pk = None
+        if not self.pks:
+            self.pks = {
+                name: True for name in self.column_names
+            }
+        if len(self.pks) == 1:
+            self.pk = next(iter(self.pks))
+        else:
+            self.pk = None
 
     def __str__(self):
         return f"{self.namespace}.{self.name}"
@@ -468,7 +472,7 @@ class Table(WithScope, Loggable):
         columns = self.order_by_alias(columns)
         # concatenate values together 
         aggregate = [f"T.{c}" for c in columns]
-        aggregate = {'json_array': aggregate}
+        aggregate = {'json_build_array': aggregate}
 
         output = []
         pk = pks[0]
@@ -594,7 +598,7 @@ class Table(WithScope, Loggable):
 
     async def get_range(self, keys=None):
         if keys is None:
-            keys = copy(self.pks) if len(self.pks) == 1 else []
+            keys = list(self.pks.keys())
             if self.on_create:
                 keys.append(self.on_create)
             if self.on_update:
@@ -602,6 +606,10 @@ class Table(WithScope, Loggable):
             keys = list(set(keys))
 
         if not keys:
+            print(f'{self.name}: no keys')
+            if self.name == 'test':
+                import pdb
+                pdb.set_trace()
             return None
 
         query = self.get_range_query(keys)
diff --git a/adbc/zql/builders/postgres.py b/adbc/zql/builders/postgres.py
index 1a9fb28..f240376 100644
--- a/adbc/zql/builders/postgres.py
+++ b/adbc/zql/builders/postgres.py
@@ -6,9 +6,6 @@ from .sql import SQLBuilder
 class PostgresBuilder(SQLBuilder):
     IDENTIFIER_QUOTE_CHARACTER = '"'
     LITERAL_QUOTE_CHARACTER = "'"
-    FUNCTION_RENAMES = {
-        'json_array': 'json_build_array'
-    }
     def get_returning(
         self, returning: Union[list, str, dict], style, params, prefix=True
     ):
diff --git a/adbc/zql/parsers/sql.py b/adbc/zql/parsers/sql.py
index 2ed5c9f..ddbbd4c 100644
--- a/adbc/zql/parsers/sql.py
+++ b/adbc/zql/parsers/sql.py
@@ -95,7 +95,28 @@ class SQLParser():
                         + Group(
                             Optional(Regex(r"\b(?:NOT\s+)NULL?\b", re.IGNORECASE))("null")
                             & Optional(Regex(r"\bAUTO(?:_)?INCREMENT\b", re.IGNORECASE))("auto_increment")
-                            & Optional(Regex(r"\b(UNIQUE|PRIMARY)(?:\s+KEY)?\b", re.IGNORECASE))("key")
+                            & (
+                                Optional(Regex(r"\b(UNIQUE|PRIMARY)(?:\s+KEY)?\b", re.IGNORECASE))("key")
+                                | (
+                                    (FOREIGN_KEY)("type")
+                                    + LPAR + Group(delimitedList(Optional(SUPPRESS_QUOTE) + Word(alphanums + "_") + Optional(SUPPRESS_QUOTE)))("constraint_columns") + RPAR
+                                    + Optional(
+                                        Suppress(REFERENCES) +
+                                        Optional(SUPPRESS_QUOTE) +
+                                        Word(alphanums + "_")("references_table") +                 
+                                        Optional(SUPPRESS_QUOTE) +
+                                        LPAR +
+                                        Group(
+                                            delimitedList(
+                                                Optional(SUPPRESS_QUOTE) + 
+                                                Word(alphanums + "_") + 
+                                                Optional(SUPPRESS_QUOTE)
+                                            )
+                                        )("references_columns") +
+                                        RPAR
+                                    )
+                                )
+                            )
                             & Optional(Regex(
                                 r"\bDEFAULT\b\s+(?:((?:[A-Za-z0-9_\.\'\" -\{\}]|[^\x01-\x7E])*\:\:(?:character varying)?[A-Za-z0-9\[\]]+)|(?:\')((?:\\\'|[^\']|,)+)(?:\')|(?:\")((?:\\\"|[^\"]|,)+)(?:\")|([^,\s]+))",
                                 re.IGNORECASE))("default")
@@ -110,7 +131,7 @@ class SQLParser():
                 |
                 COMMENT
             )
-        )("columns")
+        )("items")
 
     PARSE = Forward()
     PARSE << OneOrMore(COMMENT | CREATE_TABLE_STATEMENT)
@@ -153,7 +174,8 @@ class SQLParser():
             return None
         if 'type_name' not in type:
             raise ValueError(f'{type}: missing type name')
-        type_name = type['type_name'][0]
+
+        type_name = ' '.join(type['type_name']).lower()
         length = type.get('length')
         semantics = type.get('semantics')
         unsigned = type.get('unsigned')
@@ -186,7 +208,28 @@ class SQLParser():
         key = constraint.get('key', '')
         result['primary'] = key.upper() == 'PRIMARY KEY'
         result['unique'] = key.upper() in {'UNIQUE', 'UNIQUE KEY'}
+        if key == 'FOREIGN KEY':
+            result['related'] = {
+                'to': constraint['references_table'],
+                'by': constraint['references_columns']
+            }
         result['sequence'] = 'auto_increment' in constraint
+        result['related'] = None
+        return result
+
+    def get_constraint_type(self, type):
+        return type.lower().replace('key', '').strip()
+
+    def get_constraint_definition(self, constraint):
+        result = {}
+
+        result['name'] = column['name']
+        result['type'] = self.get_constraint_type(constraint.get('type'))
+        result['deferrable'] = column.get('deferrable', False)
+        result['deferred'] = column.get('deferred', False)
+        result['check'] = column.get('check', None)
+        result['related_name'] = column.get('references_table', None)
+        result['related_columns'] = column.get('references_columns', None)
         return result
 
     def parse_statement(self, sql=None):
@@ -214,26 +257,21 @@ class SQLParser():
         result['name'] = f'{schema}.{table}' if schema else table
         result['temporary'] = "temporary" in parsed
         result['maybe'] = 'maybe' in parsed
-        result['columns'] = {}
+        columns = []
+        constraints = []
 
-        for column in parsed["columns"]:
-            if column.getName() == "column":
+        for item in parsed["items"]:
+            if item.getName() == "column":
                 # add column
-                result['columns'][column['name']] = self.get_column_definition(column)
-
-            elif column.getName() == "constraint":
-                # set column constraint
-                constraint = column
-                type = constraint['type'].upper()
-                for column in constraint["constraint_columns"]:
-                    column = result['columns'][column]
-                    if type == "PRIMARY KEY":
-                        column['null'] = False
-                        column['primary'] = True
-                    elif type in {"UNIQUE", "UNIQUE KEY"}:
-                        column['unique'] = True
-                    elif type == "NOT NULL":
-                        column['null'] = False
-
-        result['columns'] = list(result['columns'].values())
+                # may have attached constraint
+                columns.append(self.get_column_definition(item))
+
+            elif item.getName() == "constraint":
+                # add constraint
+                constraints.append(self.get_constraint_definition(item))
+
+        # use adbc.store.Table to update the constraints/columns
+        table = Table(columns=columns, constraints=constraints)
+        result['columns'] = list(table.columns.values())
+        result['constraints'] = list(table.constraints.values())
         return {'create': {'table': result}}
diff --git a/tests/integration/test_info.py b/tests/integration/test_info.py
index 27bc416..24f712c 100644
--- a/tests/integration/test_info.py
+++ b/tests/integration/test_info.py
@@ -127,23 +127,23 @@ async def test_info():
             if type != 'sqlite':
                 # expect unique to be set 
                 expect_schema['columns']['name']['unique'] = 'unique_name'
-            else:
-                expect_schema['columns']['created']['type'] = 'timestamp'
             assert expect_schema["columns"] == actual_schema["columns"]
             assert expect_schema["constraints"] == actual_schema["constraints"]
             assert actual_data["count"] == 4
             assert actual_data["range"] == {"id": {"min": 1, "max": 6}}
-            assert actual_data['hashes'] == {1: '566991d4b9cf37367cab89ab93b74a3d'}
+            if type == 'sqlite':
+                assert actual_data['hashes'] == {1: '566991d4b9cf37367cab89ab93b74a3d'}
 
             # 9. test exclusion: ignore certain fields
-            excludes = ['unique', 'primary', 'related']
+            excludes = ['unique', 'primary', 'related', 'type']
             info = await source.get_info(
                 scope=alias_scope,
                 hashes=True,
                 exclude={'columns': excludes}
             )
             actual_schema = info['main']['test']
-            for name, column in expect_schema['columns'].items():
+            expected = deepcopy(expect_schema)
+            for name, column in expected['columns'].items():
                 for exclude in excludes:
                     column.pop(exclude)
-            assert expect_schema['columns'] == actual_schema['columns']
+            assert expected['columns'] == actual_schema['columns']
